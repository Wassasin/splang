\documentclass[11pt]{amsart}

\usepackage[british]{babel}

\usepackage{geometry}
\geometry{a4paper}
\geometry{twoside=false}

\usepackage[parfill]{parskip}
\setlength{\marginparwidth}{2cm}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{color}
\usepackage{textcomp}

\usepackage{listings}
\usepackage{prettylistings}
\usepackage{lang-llvm}
\usepackage{lang-spl}
\usepackage{lang-ssm}
\usepackage{lang-term}

\definecolor{purple}{rgb}{0.5,0.0,0.3}

\usepackage{xspace}
\usepackage{url}

\setcounter{tocdepth}{1}

\newcommand{\llvm}{\textsc{LLVM}\xspace}
\newcommand{\llvminl}[1]{\lstinline[language=llvm,basicstyle=]{#1}}

\newcommand{\spl}{\texttt{SPL}\xspace}
\newcommand{\splinl}[1]{\lstinline[language=spl,basicstyle=]{#1}}

\newcommand{\C}{\texttt{C}\xspace}
\newcommand{\Cinl}[1]{\lstinline[language=C,basicstyle=]{#1}}

\newcommand{\Cpp}{\texttt{C}$^{++}$\xspace}
\newcommand{\Cppinl}[1]{\lstinline[language=C++,basicstyle=]{#1}}

\newcommand{\ssm}{\texttt{SSM}\xspace}
\newcommand{\splang}{\textsc{Splang}\xspace}
\newcommand{\flag}[1]{\texttt{#1}}

\title{The \splang compiler}
\author{Wouter Geraedts (0814857) \and Joshua Moerman (3009048)}
\date{}

% mechanism to handle todos
\newcommand{\todo}[1]{
	\addcontentsline{tdo}{todo}{\protect{#1}}
	$\ast$ \marginpar{\tiny $\ast$ #1}
}
\makeatletter
	\newcommand \listoftodos{\section*{Todo list} \@starttoc{tdo}}
	\newcommand\l@todo[2]{
		\par\noindent \textit{#2}, \parbox{10cm}{#1}\par
	}
\makeatother


\begin{document}
\maketitle

\tableofcontents

\section{Overview}
\subsection{Getting started}
Get the source from \url{https://github.com/Wassasin/splang}\footnote{One can also \emph{view} the repository online on this address, if one is not interested in actually building and running the compiler.}. In order to compile it you'll need to install the following Haskell packages:
\begin{lstlisting}[language=term]
$ cabal update
$ cabal install ansi-terminal
$ cabal install edit-distance
$ cabal install derive
\end{lstlisting}

Then to compile \texttt{some.spl}, simply run \texttt{./run some.spl}. This will first compile the compiler (if needed), then compile \texttt{some.spl}. If you only want to tidy \texttt{some.spl} up, you can use \texttt{./parse} instead (this will provide the needed compiler flags). 

We support two targets, by default the \ssm (simple stack machine) is targeted, with the flag \flag{--target=llvm} one can target \llvm IR (which in turn can be compiled to native code with \texttt{llc}, or interpreted by \texttt{lli}). Use \texttt{./run -h} to see all compiler flags.

We extended the language by allowing external linkage (with for example \C linkage). The linking of these function can be done with \texttt{ld}. This is only possible for the \llvm target, as there is no linker for \ssm. Therefore this extension is only discussed in the section about \llvm.

\subsection{Goal and motivation}
We wanted to make a nice compiler that also provide errors and warnings in a elegant way to the user. As we like the compiler \texttt{clang} a lot, we decided to try to provide errors and warnings in the same way. This includes a clear description of where the error occurs, coloured output and in some cases even a hint on how to fix it.\footnote{Of course one might argue that the main goal of a compiler is to generate code instead of giving fancy error messages. In our eyes both aspects are very important. Good error messages will really benefit productivity.}

Another thing we wanted is \llvm output. As this is a fast growing platform with a lot of tools and libraries to help compiler writers, it is good to learn about this. As a consequence of implementing \llvm IR generation we automatically support many architectures and OSes. All in all, we hopefully justified why our compiler is called \splang.

\subsection{Compilation phases}
Our compiler uses several compilation phases. A summary is given in Table~\ref{tab:overview}. One can stop compiling after a phase by using the corresponding compiler flags \flag{--<phase>-only} and output the result after a phase with \flag{--show-<phase>}, note that not all phases have readable results. In the reading phase every error is a fatal one. In the analysis phase compilation will continue on errors (if possible). Code will only be generated for correct code, unless the \flag{--force-codegen} flag is given.

\begin{table}
\begin{tabular}{l l p{8cm}}
\multicolumn{2}{c}{Phase} & \multicolumn{1}{c}{Description}\\
\hline Reading
 & Lexing & Converts the file to a list of tokens, this strips whitespace and comments. \\
 & Parsing & Parses the list of tokens and generates the AST. \\
\hline Analysis
 & Scoping & Checks usage of identifiers, gives errors on redeclarations and undeclared identifiers. This phase rewrites the AST to contain unique identifiers. \\
 & Typing & Checks and infers types of the program. Rewrites the AST to contain types in each expression. \\
\hline Code gen
 & Templating & Rewrites functions to accommodate polymorphism. \\
 & AST to IR & Converts to the AST to our IR. \\
 & Canonicalize IR & Converts the IR into a IR consisting of basic blocks. \\
 & IR Deadcode removal & Removes basic blocks which are not reachable. \\
 & Generate \ssm & Translates the canonical IR into \ssm instructions. \\
 & Generate \llvm & Translates the canonical IR into \llvm instructions.
\end{tabular}
\caption{Overview of all the phases of the \splang compiler}
\label{tab:overview}
\end{table}


\section{Implementation}
For our SPL-compiler, \splang, we choose \emph{Haskell} as our implementation language. First of all because this is a \emph{functional programming language}, which is really helpful when defining data structures. Another nice things is the \emph{\texttt{do}-syntax} of Haskell, which makes it very easy (at least to some extend) to propagate for example error messages. Of course this part would also be very easy in a imperative language. Another nice feature is of course nested pattern-matching, which turned out to be really useful in code generation.

We depend on the libraries mentioned above (other than the library which comes with Haskell). We use \emph{ansi-terminal} for coloured output in the terminal (colour scheme is based on the {\sc Clang} compiler), \emph{edit-distance} for suggestions when the programmer typed a undeclared identifier and \emph{derive} to derive functions and type classes for some data structures.

For the \llvm IR code generation we wanted to use the \llvm Haskell bindings. But we weren't able to get this running, so we decided to do this ourselves. So we build some basic data structures to handle the aspects of \llvm we need.

\section{Parser}
We made our own parsing combinator library. It has infinite lookahead, but when performance matters there are also 1-lookahead combinators. We choose to make our own one, because then we would have full control of error messages and source code information. It is implemented in a monadic way.

\subsection{Grammar}
In order to parse the language we had to change the grammar a bit. First of all we added priorities to the different operators, so that the multiplication binds stronger than addition.

Secondly, we generalized the grammar a bit, not only function calls can be an assignment, but also expressions.

We also noted that the given grammar was ambiguous, because of the \emph{dangling else}. We didn't have to change this, nor choose an convention. Our parser has infinite lookahead, so we parse all possibilities and conclude that it is ambiguous, and throw an error.

At last there is the problem of left-recursion in the given grammar. Our parser is left-descent, so we had to do left-factoring. We also had to think about associativity of non-associative operators.

The used grammar is attached in the appendix.

\section{Scoping}
We distinguish three different scopes (from big to small): \emph{global}, \emph{function argument} and \emph{local}. If two identifiers are being declared in the same scope, an error will be thrown, but compilation will continue (to possibly catch more errors). If a identifier is being declared, but it was already declared in a bigger scope, we allow this, but we will give the user a warning (because it is probably not what you want, and might introduce subtle bugs). This is called shadowing, one can still initialize this shadowing identifier with the previous one, as shown here:

\begin{lstlisting}[language=spl]
Int x = 5; // Global
Void foo(){
    Int x = x; // ``Warning: x shadows global x''
               // x will be still initialized with the global x,
               // ie.: x == 5
}
\end{lstlisting}

Globals can be defined in any order you want. This for example allows mutual recursion. This also holds for variable declarations, so one can declare a variable using variables which are defined later. But the compiler does not make a dependency graph, so using a variable which is not defined yet, will result in using uninitialized memory.

As far as scoping is concerned variables can be used as a function, and functions can be passed to other functions. However the type system does not accept this, because the annotated type in the source can never be a function type.

\subsection{Type variable scoping}
Type variables have similar scoping rules. If two type variables are used in the same scope, they will be exactly the same type. In the following example \texttt{x} and \texttt{y} are of the same type, and therefore the second \texttt{print} will fail to type check. (We will later see another reason why this program is not allowed by the compiler).

\begin{lstlisting}[language=spl]
[t] x = [];
[t] y = []; // Same type as x

Void main(){
	print(1:x);    // => x is of type [Int]
	print(True:y); // => y is of type [Bool] => error
}
\end{lstlisting}

There is however an exception to this. We assume that if a function uses a type variable, then it is a polymorphic function. This means that type variables in function arguments are always new, in fact, this is the only way to create a new type variable with the same name. For example:

\begin{lstlisting}[language=spl]
t x = /* ... */

// The following 't' is independent of the above
[t] reverse([t] list){
	/* ... */
}
\end{lstlisting}

Note that type variables and identifiers for variables and functions live in two separate namespaces, so there is never a name clash. (For example we allow: \texttt{x x(x x)\{return x;\}}, which is the identity function.)

\section{Typing}

Our compiler mainly does type checking, but some type inference is also needed. Type inference is hard to define in a imperative language, as the variables one is inferring, might be used differently later on (and mutual recursion is hard). This led to a system which does both type inference and type checking.

We first determine the annotated types of every global variable and function. Then in all expressions and function bodies those annotations are used to infer the actual type at every place in the expression or function body, and with this information the annotated type is checked. We also make sure that \texttt{return} statements really return what they should, and if there is no \texttt{return} statement, the function should return \texttt{Void}.

We only allow \texttt{Void} to occur as a return type of a function. So a list or tuples of \texttt{Void} will give an error. One also cannot supply \texttt{Void} as argument, even when the function is polymorphic. For example \texttt{print(print(5))} will not compile for this reason.

We do not allow the programmer to write this:

\begin{lstlisting}[language=spl]
t x = 5;
\end{lstlisting}

Because it is weird to annotate x with a more general type than it really is. This is a explicit decision we made, technically the compiler is able to infer that \texttt{t == Int}. Allowing this code for variables might seem reasonable, but allowing this also for functions is wrong, as can be seen in the following example:

\begin{lstlisting}[language=spl]
Bool x = f();

// This should not be allowed
t f(){ return 5; }
\end{lstlisting}

Type checking the assignment of the global variable \texttt{y} is ok, since \texttt{f} returns everything you want. So we should not allow the type annotation given to the function \texttt{f}. For consistency we also enforce this for variables. We might at some point in the future allow this flexibility for variables.

\subsection{Formal rules}

\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\s}{\ast}
\newcommand{\sn}[1]{\s_{#1}}

As explained before we use the type annotation to support mutual recursion. To check the correctness of this annotation the type is inferred. If an annotation is not as concrete as the actual type of a declaration an error should be displayed. Between this inferred and annotated type a bijection should exist for every free type variable.

For type inferencing we use the following functions:

\begin{itemize}
	\item Type Unification: \\
		$\U \colon \mbox{Type} \rightarrow \mbox{Type} \rightarrow \mbox{Substitution}$
	\item Type Inferencing for declarations: \\
		$\T_{\mbox{d}} \colon \mbox{Context} \rightarrow \mbox{Decl} \rightarrow \mbox{Substitution}$
	\item Type Inferencing for statements and expressions: \\
		$\T \colon \mbox{Context} \rightarrow \mbox{Decl} \rightarrow \mbox{Type} \rightarrow \mbox{Substitution}$
\end{itemize}

We define $\U$ as the function stated in the course slides, and $\T_{\mbox{d}}$ and $\T$ as following:

$\begin{array}{rcl}
	\T_{\mbox{d}}(\Gamma, \mbox{Program}~\vec{d}) & = & \s_n \\
	\sn{n} & = & \T_{\mbox{d}}(\Gamma^{\sn{n-1}}, d_n) \circ \sn{n-1} \\
	& \vdots & \\
	\sn{2} & = & \T_{\mbox{d}}(\Gamma^{\sn{1}}, d_2) \circ \sn{1} \\
	\sn{1} & = & \T_{\mbox{d}}(\Gamma, d_1) \\
	
	\\
	
	\T_{\mbox{d}}(\Gamma, \mbox{VarDecl}~t~x~e) & = & \T(\Gamma, e, \tau) \\
	\forall \emptyset . \, \tau	& = & \Gamma(x) \\

	\\

	\T_{\mbox{d}}(\Gamma, \mbox{FunDecl}~t~x~\vec{\mbox{arg}}~\vec{\mbox{decl}}~\vec{\mbox{stmt}}) & = & \U(\theta^{\sn{1}}, \upsilon^{\sn{1}}) \circ \sn{1} \\
	\upsilon	& = & \vec{\alpha}^{\sn{1}} \rightarrow \beta^{\sn{1}} \\
	\sn{1}		& = & \sn{(2,n)} \\
	\sn{(2,n)}	& = & \T(\Gamma'^{\sn{(2,n-1)}}, \mbox{stmt}_n, \beta^{\sn{(2,n-1)}}) \circ \sn{(2,n-1)} \\
	& \vdots & \\
	\sn{(2,2)}	& = & \T(\Gamma'^{\sn{(2,1)}}, \mbox{stmt}_2, \beta^{\sn{(2,1)}}) \circ \sn{(2,1)} \\
	\sn{(2,1)}	& = & \T(\Gamma'^{\sn{3}}, \mbox{stmt}_1, \beta^{\sn{3}}) \circ \sn{(3,n)} \\
	\sn{(3,n)}	& = & \T_{\mbox{d}}(\Gamma'^{\sn{(3,n-1)}}, \mbox{decl}_n) \circ \sn{(3,n-1)} \\
	& \vdots & \\
	\sn{(3,2)}	& = & \T_{\mbox{d}}(\Gamma'^{\sn{(3,1)}}, \mbox{decl}_2) \circ \sn{(3,1)} \\
	\sn{(3,1)}	& = & \T_{\mbox{d}}(\Gamma'^{\sn{4}}, \mbox{decl}_1) \circ \sn{4} \\
	\Gamma'		& = & \Gamma' [\mbox{arg}_n.x := \alpha_n] \\
	& \vdots & \\
	\Gamma'		& = & \Gamma' [\mbox{arg}_2.x := \alpha_2] \\
	\Gamma'		& = & \Gamma' [\mbox{arg}_1.x := \alpha_1] \\
	& & \mbox{where}~\beta~\mbox{fresh} \\
	& & \mbox{and}~\vec{\alpha}~\mbox{fresh for all~} \vec{\mbox{arg}} \\
\end{array}$

$\begin{array}{rcl}
	\T(\Gamma, \mbox{Expr}~e, \sigma) & = & \T(\Gamma, e, \alpha) \\
	& & \mbox{where}~\alpha~\mbox{fresh} \\

	\\

	\T(\Gamma, \mbox{Scope}~\vec{\mbox{stmt}}, \sigma) & = & \sn{n} \\
	\sn{n} & = & \T(\Gamma^{\sn{n-1}}, \mbox{stmt}_n, \sigma^{\sn{n-1}}) \circ \sn{n-1} \\
	& \vdots & \\
	\sn{2} & = & \T(\Gamma^{\sn{1}}, \mbox{stmt}_2, \sigma^{\sn{1}}) \circ \sn{1} \\
	\sn{1} & = & \T(\Gamma, \mbox{stmt}_1, \sigma) \\

	\\

	\T(\Gamma, \mbox{If}~e~\mbox{stmtt}, \sigma) & = & \T(\Gamma^{\s}, \mbox{stmtt}, \sigma^{\s}) \circ \s \\
	\s & = & \T(\Gamma, e, \mbox{Bool}) \\

	\\

	\T(\Gamma, \mbox{IfElse}~e~\mbox{stmtt}~\mbox{stmte}, \sigma) & = & \T(\Gamma^{\s}, \mbox{stmte}, \sigma^{\s}) \circ \s \\
	\s & = & \T(\Gamma^{\sn{1}}, \mbox{stmtt}, \sigma^{\sn{1}}) \circ \sn{1} \\
	\sn{1} & = & \T(\Gamma, e, \mbox{Bool}) \\

	\\

	\T(\Gamma, \mbox{While}~e~\mbox{stmt}, \sigma) & = & \T(\Gamma^{\s}, \mbox{stmt}, \sigma^{\s}) \circ \s \\
	\s & = & \T(\Gamma, e, \mbox{Bool}) \\

	\\

	\T(\Gamma, \mbox{Assignment}~i~e, \sigma) & = & \T(\Gamma, \mbox{e}, \tau [\vec{\alpha} := \vec{\beta}] ) \\
	\forall \vec{\alpha} .\, \tau & = & \Gamma(i) \\
	& & \mbox{where}~\vec{\beta}~\mbox{fresh} \\

	\\

	\T(\Gamma, \mbox{Return}, \sigma) & = & \U(\sigma, \mbox{Void}) \\

	\\

	\T(\Gamma, \mbox{Return}~e, \sigma) & = & \U(\sigma^{\s}, \alpha^{\s}) \circ \s \\
	\s & = & \T(\Gamma, e, \alpha) \\
	& & \mbox{where}~\alpha~\mbox{fresh} \\
\end{array}$

$\begin{array}{rcl}
	\T(\Gamma, \mbox{Var}~i, \sigma) & = & \U(\sigma, \tau [\vec{\alpha} := \vec{\beta}] ) \\
	\forall \vec{\alpha} .\, \tau & = & \Gamma(i) \\
	& & \mbox{where}~\vec{\beta}~\mbox{fresh} \\
	
	\\
	
	\T(\Gamma, e_1 \odot e_2, \sigma) & = & \U(\sigma^{\s}, \tau^{\s}) \\
	\s	& = & \T(\Gamma^{\sn{1}}, e_2, \gamma_2^{\sn{1}}) \circ \sn{1} \\
	\sn{1}	& = & \T(\Gamma, e_1, \gamma_1) \\
	& & \mbox{where}~\odot : \gamma_1 \rightarrow \gamma_2 \rightarrow \tau \\
	
	\\
	
	\T(\Gamma, \boxdot e, \sigma) & = & \U(\sigma^{\s}, \tau^{\s}) \\
	\s	& = & \T(\Gamma, e, \gamma) \\
	& & \mbox{where}~\boxdot : \gamma \rightarrow \tau \\
	
	\\
	
	\T(\Gamma, \mbox{Int}~i, \sigma) & = & \U(\sigma, \mbox{Int}) \\

	\\
	
	\T(\Gamma, \mbox{Bool}~b, \sigma) & = & \U(\sigma, \mbox{Bool}) \\
	
	\\
	
	\T(\Gamma, f(e_1, \hdots, e_n), \sigma) & = & \s \\
	\s		& = & \U(\sigma^{\sn{(1, n)}}, \rho^{\sn{(1, n)}}) \circ \sn{(1, n)} \\
	\sn{(1, n)}	& = & \T(\Gamma^{\sn{(1, n-1)}}, e_n, \pi_n^{\sn{(1, n-1)}}) \circ \sn{(2, n-1)} \\
	& \vdots & \\
	\sn{(1, 2)}	& = & \T(\Gamma^{\sn{(1, 1)}}, e_2, \pi_2^{\sn{(1, 1)}}) \circ \sn{(1, 1)} \\
	\sn{(1, 1)}	& = & \T(\Gamma^{\sn{2}}, e_1, \pi_1^{\sn{2}}) \circ \sn{2} \\
	\sn{2}		& = & \U(\tau[\vec{\alpha} := \vec{\beta}], \pi_1 \rightarrow \hdots \rightarrow \pi_n \rightarrow \rho) \\
	\forall \vec{\alpha} .\, \tau & = & \Gamma(f) \\
	& & \mbox{where}~\vec{\alpha}, \vec{\beta}, \vec{\pi}, \rho~\mbox{fresh} \\
	
	\\
	
	\T(\Gamma, (e_1, e_2), \sigma)	& = & \U(\sigma^{\s}, (\alpha_1, \alpha_2)^{\s}) \circ \s \\
	\s				& = & \T(\sigma^{\sn{1}}, e_2, \alpha_2^{\sn{1}}) \circ \sn{1} \\
	\sn{1}				& = & \T(\sigma, e_1, \alpha_1) \\
	& & \mbox{where}~\alpha_1, \alpha_2~\mbox{fresh} \\
	
	\\
	
	\T(\Gamma, [], \sigma)	& = & \U(\sigma, [\alpha]) \\
	& & \mbox{where}~\alpha~\mbox{fresh} \\
\end{array}$

\section{Semantics}
The semantics are straight forward. Integer arithmetic is the usual arithmetic, with overflow behaviour and division rounding defined by the target (i.e. by the \ssm). Also the boolean operators work as usual. We don't do short-circuiting, so in the following code \texttt{f} will be called:
\begin{lstlisting}[language=spl]
if(False && f()){
	// ...
}
\end{lstlisting}

If one calls multiple statements in one statements, generally those functions will be called from left to right, unless one of the functions is nested in another function call. So in the following example \texttt{a}, \texttt{b} and \texttt{c} are called (in this order) before \texttt{f}:
\begin{lstlisting}[language=spl]
	Int x = f(a(), b(), c());
\end{lstlisting}

All functions take arguments by value. There are no mechanism to update a data structure (i.e. tuple or list). So one always has to construct such an object fully.

\subsection{Polymorphism}

Polymorphic functions will be instantiated for each use. Starting from the \texttt{main} function, and all global variable declarations, all function calls are rewritten to point to a specific version for a specific type of the called function. These functions along with the type for which these functions are called are added to a queue. Every declaration in this queue is then rewritten in a similar way. This is like the templating system in C++. We decided on a templating system because we also want to target \llvm IR, which is strongly typed (without polymorphism). This also means that we do not require boxing for complex types such as tuples.

Rewritten functions use a mangled name describing the type for which the function is called. In the following case, for example:

\begin{lstlisting}[language=spl]
Void f(a x){
	print(x);
}

Void main(){
	f(42);
	f(True);
	f(((1, 2), (3, 4)));
}
\end{lstlisting}

Because \texttt{f} is called for three separate types, the function is instantiated three times:

\begin{lstlisting}[language=spl]
// main_v :: Void()
Void main_v(){
	f_v_i(42);
	f_v_b(True);
	f_v_ppiipii(((1, 2), (3, 4)));
}
// f_v_i :: Void(Int)
Void f_v_i(a x){
	print(x);
}
// f_v_b :: Void(Bool)
Void f_v_b(a x){
	print(x);
}
// f_v_ppiipii :: Void(((Int, Int), (Int, Int)))
Void f_v_ppiipii(a x){
	print(x);
}
\end{lstlisting}

Of course we can easily write a program which requires infinitely many types, for example:

\begin{lstlisting}[language=spl]
Void f(t x){
	f((x,x));
}

Void main(){
	f(5);
}
\end{lstlisting}

However such programs will never terminate, so they are not very useful in practice. In order to guarantee that the \splang compiler terminates, we will only instantiate until a certain depth is reached. We regard this as a small drawback, but the advantage of this is that we know the exact concrete types when generating code.

\section{\ssm}
% what should this chapter contain:
% A chapter explaining the compilation schemes used in your compiler. A
% concise informal or semiformal description suï¬ƒces, so a formal description
% of the compilation scheme is welcome, but not required. Typical things to
% 5explain here are calling conventions, stack management, stack layout, heap
% layout, and heap management.

In this section we talk about the compilation schemes for the \ssm target. Most instructions only use the stack. There is also a heap, and only some registers. All data is in terms of \emph{word}s. A word is the smallest addressable unit. Integers and bools are stored as one word, as all instructions assume this (there is also no mechanism for bigger integers, since there is no carry/overflow flag).
When we talk about the \emph{size} of an object, we mean the number of words it takes to store it.

\subsection{Tuples and lists}
Tuples will be ``flattened'', and used as usual objects on the stack. This is possible because we always know the exact concrete type which is being used, and hence the exact size (i.e.: number of words) of such a tuple. So for example for the tuple \texttt{(1, (2, 3))} the values 1, 2 and 3 will be pushed on the stack (in this order), the operations \texttt{fst} and \texttt{snd} are now very simple. The first one simply adjusts the stack pointer (this discards the second part), and the latter copies the last part on the first part, leaving only the second part.

Lists are heap-based objects, since its size is unknown at compile time (unlike the tuples). Lists are implemented as linked lists. Internal nodes consist of a pointer to the next node and the value (which can be an object bigger than one word). This makes the size of one node 1 plus the size of the value contained by the node. The list itself is then a pointer to the first node (or 0, if the list is empty).

The heap is not garbage collected. So lists are leaked. Tuples are naturally garbage collected, as they are stack-based objects.

\subsection{Calling convention and stack management}
When calling a function, the caller allocates space on the stack for the return value (if any) and then pushes the arguments on the stack, in normal order (first arguments first). Then the function is called. The callee is allowed to change these arguments in its calculation and copies the result to the allocated space. It's then the callers job to clean up the pushed arguments, this leaves only the return value (if any) on top of the stack.

Note that this scheme makes it possible to return values which are bigger than one word (which is not possible using the return register), this was needed to return tuples.

For the actual call the \texttt{bsr} and \texttt{ret} instructions are used. So on top of the arguments there will be a return address. The callee then also pushes the mark pointer (MP) on the stack and updates the MP to the current stack pointer (SP). The callee uses the MP to clean up any local variables, by setting SP to MP. On return the callee resets the original MP.

Global variables are stored at the beginning of the stack (before \texttt{main} is entered). In order to know where the stack begins, we store the initial stack pointer to register 5. All global variables are initialized with 0. This means that lists are initialized with an empty list, integers with 0, and tuples are constructed with all zeros (see the next subsection about tuples). After this initialization, the real initialization begins (in order of declaration), then \texttt{main} is called. After executing the program, these values are not cleaned up.

\subsection{Printing}
The \texttt{print} function will do a shallow print. For lists this means it only print the pointer to the first element. For tuples this means it will print all the values in the tuple, but in reverse order (due to technical reasons and efficiency). We think printing lists and tuples incorrectly is not really an issue for now, since printing on the \ssm is very limited anyways (it is for example impossible to print brackets). Integers are printed as they should, bools are printed as: -1 meaning true, 0 meaning false.


\section{\llvm}
\subsection{Typical usage}
As said, the \splang compile can target \llvm IR. In order to run this code one should have \llvm installed. You can install llvm from the package manager, or get it from the official site (or github). \llvm comes with a number of tools, for compiling, optimizing and interpreting \llvm IR. A typical usage is:
\begin{lstlisting}[language=term]
$ splang --target=llvm my_file.spl | opt -O3 | llc -filetype=obj > my_file.o
$ ld -native -o my_file my_file.o
$ ./my_file
\end{lstlisting}
The first command compiles your \texttt{.spl}-file to \llvm IR, which is fed into the \llvm optimizer \texttt{opt} and then compiled into an object file with \texttt{llc}. This object file has then to be linked with for example \texttt{libc} and possibly other OS-dependent libraries. The outcome is a native binary \texttt{my\_file}.

\subsection{\llvm}
The advantage of this approach of native code generation is that \llvm can do the optimization (for the specific machine we run it on).

\subsection{External linkage}
We support calling external functions from a \spl-file. In order to do this one should first declare these external functions with the \splinl{extern} keyword. For example:
\begin{lstlisting}[language=spl]
extern "C" Int get_int_from_user();
\end{lstlisting}
Defines a function with \C linkage. It is also possible to link separate \spl-files, in that case use \splinl{extern "SPL"} instead. Note that we need to make this distinction because \splang does name-mangling, whereas \C doesn't do so.

With this feature we get I/O and better printing facilities for \spl.

An important limitation of this feature is that \splang does not accept polymorphic functions with external linkage, since all types are concrete at any time (due to templating).

Also note that due to templating code will only be generated for reachable functions (from \splinl{main}). In order to generate code for other functions (for example in the absence of \splinl{main}), one can declare that the compiler should export the function:
\begin{lstlisting}[language=spl]
export [Int] sort([Int] n){
	// sorting magic here
}
\end{lstlisting}

\subsection{Types}
As \llvm IR is strongly typed we have to emit types for the types used in the \spl-program. Here we will discuss how \spl types are mapped to the \llvm types. The types \splinl{Int} and \splinl{Bool} are mapped to \llvminl{i32} (an integral type with 32 bits) and \llvminl{i1} (an integral type with 1 bit) respectively. Note that all arithmetic uses signed integer semantics.

Tuples are translated to structs, so for example \splinl{(Bool, Int)} becomes \llvminl{\{i1, i32\}}. The alignment of this struct is the default alignment used by the \llvm compiler.

Lists are singly linked lists. A node consists of a pointer to the next element and then the value. So for example a list of integers will be translated to \llvminl{\%li = type \{\%li*, i32\}}.

If one wants to call \spl-functions from a \C file, he or she should be able to mimic this convention with \C-structs. For \Cpp we created a simple header which defines struct templates which are compatible with the code our compiler outputs. For example \Cppinl{spl::pair<bool, int>} defines the pair type from above and \Cppinl{spl::list<int>} defines a list of integers.

\subsection{Examples}
In our repository one can find examples on how to use this linking. We show that it is quite easy to call \C functions from within a \spl file and vice versa. One can find the following examples in the \texttt{tests/llvm} directory.

\begin{itemize}
	\item[externc] This shows that it is easy to define a function in \C to be used in a \spl program. In this case we use \C to ask the user to enter numbers. In the \spl file we store these numbers in a list, and then use the standard \C function \Cinl{rand} to shuffle this list. The shuffled list is outputted.
	\item[externspl] Here we show that we have separate compilation for \spl files. We defined two sorting algorithms (mergesort and insertionsort) in different files. In the main source file we only have to use them. This leaves the main file small and tidy.
	\item[cpp] This shows that we can also do the converse. We can call \spl functions from a \Cpp source (we chose to do \Cpp here, because we aren't really good at \C). We implemented two simple list algorithms in \spl (reverse and zip) and use them in \Cpp. This shows that we can use the convenient list syntax from \spl and still do all the other things in a language we like.
	\item[performance] Of course we would like to know what we gain from \llvm. We implemented the fibonacci sequence with a double recursion (the naive way to do this). We see that the \ssm interpreter needs 8 seconds to print the first 20. Whereas the native binary\footnote{both unoptimized and optimized} only uses 0.003 seconds. Looking at the optimized code we see that \llvm got rid of one recursive call and uses a loop instead.

	How does it compete against other compilers? We also implemented the ackermann function. The unoptimized native binary takes 21 seconds to complete the task, the optimized version only 12 seconds (so indeed we get some nice optimization from \llvm). In comparison \texttt{gcc} generates code which completes in only 8 seconds, and \texttt{clang} even manages to get a stunning 4 seconds.\footnote{Times were measured on a early 2011 13'' MacBook Pro (2.7 GHz Intel i7).}
\end{itemize}

\section{Tests}
All tests of the first phases are performed with the \texttt{--show-input --show-stages --type-only} flags, and all colors are stripped. These tests show the error-messages on wrong input, and a pretty printed version of the program augmented with both the scoping results (after every identifier), and typing (before every declaration).

\subsection{Parsing}
\begin{itemize}
	\item[fail\_ambi] This program can be read ambiguously. Note that the compiler detects this, and prints out all possible interpretations. However this error is not fatal, and the compilation continues, and catches a typing error: the \texttt{if}-construct expects a \texttt{Bool}, but an \texttt{Int} was given.
	\item[pass\_parser] This program shows that the compiler correctly handles associativity and priorities of infix operators.
\end{itemize}
\subsection{Scoping}
\begin{itemize}
	\item[warn\_shadowing] This shows the warnings one get when redeclaring the same identifier in a more specific scope. Note that the functions \texttt{x} and \texttt{y} are both identity functions.
	\item[fail\_identifier\_errors] If one redeclares identifiers in the same scope, an non-fatal error will be given. Note that compilation continues and more errors are found. If an identifiers is undeclared, a suggestion is given by the compiler.
\end{itemize}
\subsection{Typing}
\begin{itemize}
	\item[fail\_arguments] Shows the error messages you get when you don't supply the right amount of arguments. Also note that the arguments that \emph{are} supplied, are also type checked.
	\item[fail\_void\_no\_return] Shows that we do not accept a returning function without return statement. Specifically, we see that the compiler infers that \texttt{foo} returns \texttt{Void}, instead of \texttt{Int}. Furthermore we see that we cannot use values of type \texttt{Void}.
	\item[pass\_merge\_sort] Shows that the compiler is capable of handling polymorphism. And is nicely shows a real world example, and also shows mutual recursion.
	\item[pass\_reverse] Shows a basic reverse function. It is also used on a list of lists.
	\item[fail\_empty\_list] Shows our compilers output on the \emph{Empty list} example discussed above. Note that the type \texttt{t} is used in different ways (both \texttt{Int} and \texttt{Bool}).
	\item[pass\_polymorphism] Shows polymorphism and also scoping of type variables.
\end{itemize}

\subsection{Code generation}
To test code generation we run the generated code in the \ssm interpreter and also execute the native binaries generated with \llvm. We use a gui-less version of the \ssm interpreter which can be downloaded from \url{https://github.com/mklinik/ssm-nogui}. The result is then checked against the expected output, the following examples all pass this test. For the full test-case we refer to our repository at \url{https://github.com/Wassasin/splang}, in particular one can have a look at the script \texttt{run\_tests}.

\begin{itemize}
	\item[pass\_fib] Implement two versions of the Fibonacci sequence. One with recursion, one with a single loop. So this tests simple arithmetic, and also simple tuples. We see that the recursive version is slow for inputs larger than 10, whereas the looped version is not.
	\item[pass\_order] Tests the order of evaluation
	\item[pass\_lists] A more interesting test, involving lists of tuples. This tests some polymorphic operations on lists, as length, reverse and zip.
	\item[pass\_merge\_sort] As above, the merge sort function.
\end{itemize}

\section{Reflection}
In the first phase Wouter made the lexer, and Joshua made a first implementation of the parser in \emph{Parsec}, however we decided to write our own parsing library. This was mainly done by Wouter. In the second phase Joshua mainly worked on the scoping and type annotations, Wouter mainly on the type inference.

In the third phase Wouter mainly worked on templating, while Joshua mainly worked on the transformation of the AST to the IR, and the transformation of the IR to \ssm. For the \llvm output Joshua began with a stub, which later was extended and finished by Wouter, while Joshua made some testcases and demos. All design choices were discussed together to ensure a working whole. And also in some cases one found a bug in the others code. So eventually all code understood by both of us.

For a very precise description of who did what, one can have a look at our repository:

\url{https://github.com/Wassasin/splang}

\newpage
\appendix
\section{Grammar}

\newcommand{\tok}[1]{`\texttt{#1}'}
\newcommand{\I}{\hspace{0.1cm}$\mid$\hspace{0.2cm}}

\begin{tabular}[t]{p{2.5cm} c p{10cm}}
Prog		& := & Decl$^+$					\\
Decl		& := & ExtDecl \I VarDecl \I FunDecl		\\
VarDecl		& := & Type id \tok{=} Exp \tok{;}		\\
FunDecl		& := & [\tok{export}] RetType id \tok{(} [ FArgs ] \tok{)} \tok{\{} VarDecl* Stmt$^+$ \tok{\}} \\
ExtDecl		& := & \tok{extern} lang RetType id \tok{(} [ FArgs ] \tok{)} \tok{;} \\
RetType		& := & Type \I \tok{Void}			\\
Type		& := & \tok{Int} \I \tok{Bool} \I id		\\
		& \I & \tok{(} Type \tok{,} Type \tok{)}	\\
		& \I & \tok{[} Type \tok{]}			\\
FArgs		& := & Type id \I Type id \tok{,} Fargs		\\
&&\\
Stmt		& := & \tok{\{} Stmt* \tok{\}}			\\
		& \I & \tok{if} \tok{(} Exp \tok{)} Stmt	\\
		& \I & \tok{if} \tok{(} Exp \tok{)} Stmt \tok{else} Stmt \\
		& \I & \tok{while} \tok{(} Exp \tok{)} Stmt 	\\
		& \I & id \tok{=} Exp \tok{;}			\\
		& \I & Exp \tok{;}				\\
		& \I & \tok{return} Exp \tok{;}			\\
&&\\
Exp		& := & Term0					\\
Term0		& := & Term1 \I Term1 Op2Bool Term0		\\
Term1		& := & Term2 \I Term2 Op2Equal Term1		\\
Term2		& := & Term3 \I OpNot Term2			\\
Term3		& := & Term4 \I Term4 OpCons Term3		\\
Term4		& := & Term5 \I Term5 Term4b			\\
Term4b		& := & Op2Add Term5 Term4b \I $\epsilon$	\\
Term5		& := & Term6 \I Term6 Term5b			\\
Term5b		& := & Op2Mult Term6 Term5b \I $\epsilon$	\\
Term6		& := & OpNegative Term6 \I Term7		\\
Term7		& := & int					\\
		& \I & \tok{(} Exp \tok{)}			\\
		& \I & \tok{(} Exp \tok{,} Exp \tok{)}		\\
		& \I & \tok{False}				\\
		& \I & \tok{True}				\\
		& \I & id					\\
		& \I & FunCall					\\
		& \I & \tok{[]}					\\
&&\\
FunCall		& := & id \tok{(} [ ActArgs ] \tok{)}		\\
ActArgs		& := & Exp \I Exp \tok{,} ActArgs		\\
&&\\
Op2Mult		& := & \tok{$\ast$} \I \tok{/} \I \tok{\%} 	\\
Op2Add		& := & \tok{+} \I \tok{-}			\\
Op2Cons		& := & \tok{:}					\\
Op2Equal	& := & \tok{==} \I \tok{<} \I \tok{>} \I \tok{<=} \I \tok{>=} \I \tok{!=} \\
Op2Bool		& := & \tok{\&\&} \I \tok{||}			\\
&&\\
OpNot		& := & \tok{!}					\\
OpNegative	& := & \tok{-}					\\
&&\\
int		& := & digit$^+$				\\
id		& := & alpha \I alpha id'			\\
id'		& := & id' \tok{\_} \I id' alphaNum
\end{tabular}

%DAFUQ it doesn't work :(:(:(
%\listoftodos

\end{document}
